<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>GenAI Crew</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="A collection of GenAI-related posts"><meta name=generator content="Hugo 0.146.0"><meta name=robots content="index, follow"><meta name=author content="GenAI Crew"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link href=/tags/index.xml rel=alternate type=application/rss+xml title="GenAI Crew"><link href=/tags/index.xml rel=feed type=application/rss+xml title="GenAI Crew"><link rel=canonical href=https://juananpe.github.io/tags/><meta property="og:url" content="https://juananpe.github.io/tags/"><meta property="og:site_name" content="GenAI Crew"><meta property="og:title" content="Tags"><meta property="og:description" content="A collection of GenAI-related posts"><meta property="og:locale" content="en_us"><meta property="og:type" content="website"><meta itemprop=name content="Tags"><meta itemprop=description content="A collection of GenAI-related posts"><meta itemprop=datePublished content="2025-05-06T00:00:00+00:00"><meta itemprop=dateModified content="2025-05-06T00:00:00+00:00"><meta name=twitter:card content="summary"><meta name=twitter:title content="Tags"><meta name=twitter:description content="A collection of GenAI-related posts"></head><body class="ma0 avenir bg-near-white production"><header><div class="pb3-m pb6-l bg-black"><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=/ class="f3 fw2 hover-white white-90 dib no-underline">GenAI Crew</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="Home page">Home</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/archives/ title="Archives page">Archives</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/tags/ title="Tags page">Tags</a></li></ul><div class=ananke-socials></div></div></div></nav><div class="tc-l pv3 ph3 ph4-ns"><h1 class="f2 f-subheadline-l fw2 light-silver mb0 lh-title">Tags</h1></div></div></header><main class=pb7 role=main><article class="cf pa3 pa4-m pa4-l"><div class="measure-wide-l center f4 lh-copy nested-copy-line-height nested-links mid-gray"></div></article><div class="mw8 center"><section class=ph4><h2 class=f1><a href=/tags/ai-tools/ class="link blue hover-black">Tag: AI Tools</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 6, 2025</div><h1 class="f3 near-black"><a href=/posts/gradio-mcp-support/ class="link black dim">Gradio MCP Support: Building AI Tools in Just 5 Lines of Code</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><h2 id=gradio-now-supports-the-model-context-protocol-mcp>Gradio Now Supports the Model Context Protocol (MCP)</h2><p><a href=https://www.gradio.app/>Gradio</a>, the popular Python library for building ML interfaces, now officially supports the Model Context Protocol (MCP). This means any Gradio app can be called as a tool by Large Language Models (LLMs) like Claude and GPT-4.</p><h2 id=what-is-mcp>What is MCP?</h2><p>The Model Context Protocol standardizes how applications provide context to LLMs. It allows models to interact with external tools such as image generators, file systems, and APIs. By providing a standardized protocol for tool-calling, MCP extends LLMs&rsquo; capabilities beyond text generation.</p></div><a href=/posts/gradio-mcp-support/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/gradio/ class="link blue hover-black">Tag: Gradio</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 6, 2025</div><h1 class="f3 near-black"><a href=/posts/gradio-mcp-support/ class="link black dim">Gradio MCP Support: Building AI Tools in Just 5 Lines of Code</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><h2 id=gradio-now-supports-the-model-context-protocol-mcp>Gradio Now Supports the Model Context Protocol (MCP)</h2><p><a href=https://www.gradio.app/>Gradio</a>, the popular Python library for building ML interfaces, now officially supports the Model Context Protocol (MCP). This means any Gradio app can be called as a tool by Large Language Models (LLMs) like Claude and GPT-4.</p><h2 id=what-is-mcp>What is MCP?</h2><p>The Model Context Protocol standardizes how applications provide context to LLMs. It allows models to interact with external tools such as image generators, file systems, and APIs. By providing a standardized protocol for tool-calling, MCP extends LLMs&rsquo; capabilities beyond text generation.</p></div><a href=/posts/gradio-mcp-support/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/llm/ class="link blue hover-black">Tag: Llm</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 6, 2025</div><h1 class="f3 near-black"><a href=/posts/gradio-mcp-support/ class="link black dim">Gradio MCP Support: Building AI Tools in Just 5 Lines of Code</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><h2 id=gradio-now-supports-the-model-context-protocol-mcp>Gradio Now Supports the Model Context Protocol (MCP)</h2><p><a href=https://www.gradio.app/>Gradio</a>, the popular Python library for building ML interfaces, now officially supports the Model Context Protocol (MCP). This means any Gradio app can be called as a tool by Large Language Models (LLMs) like Claude and GPT-4.</p><h2 id=what-is-mcp>What is MCP?</h2><p>The Model Context Protocol standardizes how applications provide context to LLMs. It allows models to interact with external tools such as image generators, file systems, and APIs. By providing a standardized protocol for tool-calling, MCP extends LLMs&rsquo; capabilities beyond text generation.</p></div><a href=/posts/gradio-mcp-support/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/survey-ai-agent-protocols/ class="link black dim">A Survey of AI Agent Protocols: Framework and Future</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>A recent research paper from Shanghai Jiao Tong University and the ANP Community provides the first comprehensive analysis of existing agent protocols, offering a systematic two-dimensional classification that distinguishes between context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.</p><p>The paper highlights a critical issue in the rapidly evolving landscape of LLM agents: the lack of standardized protocols for communication with external tools or data sources. This standardization gap makes it difficult for agents to work together effectively or scale across complex tasks, ultimately limiting their potential for tackling real-world problems.</p></div><a href=/posts/survey-ai-agent-protocols/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/mcp/ class="link blue hover-black">Tag: MCP</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 6, 2025</div><h1 class="f3 near-black"><a href=/posts/gradio-mcp-support/ class="link black dim">Gradio MCP Support: Building AI Tools in Just 5 Lines of Code</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><h2 id=gradio-now-supports-the-model-context-protocol-mcp>Gradio Now Supports the Model Context Protocol (MCP)</h2><p><a href=https://www.gradio.app/>Gradio</a>, the popular Python library for building ML interfaces, now officially supports the Model Context Protocol (MCP). This means any Gradio app can be called as a tool by Large Language Models (LLMs) like Claude and GPT-4.</p><h2 id=what-is-mcp>What is MCP?</h2><p>The Model Context Protocol standardizes how applications provide context to LLMs. It allows models to interact with external tools such as image generators, file systems, and APIs. By providing a standardized protocol for tool-calling, MCP extends LLMs&rsquo; capabilities beyond text generation.</p></div><a href=/posts/gradio-mcp-support/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/model-context-protocol/ class="link blue hover-black">Tag: Model Context Protocol</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 6, 2025</div><h1 class="f3 near-black"><a href=/posts/gradio-mcp-support/ class="link black dim">Gradio MCP Support: Building AI Tools in Just 5 Lines of Code</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><h2 id=gradio-now-supports-the-model-context-protocol-mcp>Gradio Now Supports the Model Context Protocol (MCP)</h2><p><a href=https://www.gradio.app/>Gradio</a>, the popular Python library for building ML interfaces, now officially supports the Model Context Protocol (MCP). This means any Gradio app can be called as a tool by Large Language Models (LLMs) like Claude and GPT-4.</p><h2 id=what-is-mcp>What is MCP?</h2><p>The Model Context Protocol standardizes how applications provide context to LLMs. It allows models to interact with external tools such as image generators, file systems, and APIs. By providing a standardized protocol for tool-calling, MCP extends LLMs&rsquo; capabilities beyond text generation.</p></div><a href=/posts/gradio-mcp-support/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/agent-frameworks/ class="link blue hover-black">Tag: Agent-Frameworks</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/survey-ai-agent-protocols/ class="link black dim">A Survey of AI Agent Protocols: Framework and Future</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>A recent research paper from Shanghai Jiao Tong University and the ANP Community provides the first comprehensive analysis of existing agent protocols, offering a systematic two-dimensional classification that distinguishes between context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.</p><p>The paper highlights a critical issue in the rapidly evolving landscape of LLM agents: the lack of standardized protocols for communication with external tools or data sources. This standardization gap makes it difficult for agents to work together effectively or scale across complex tasks, ultimately limiting their potential for tackling real-world problems.</p></div><a href=/posts/survey-ai-agent-protocols/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/agent-protocols/ class="link blue hover-black">Tag: Agent-Protocols</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/survey-ai-agent-protocols/ class="link black dim">A Survey of AI Agent Protocols: Framework and Future</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>A recent research paper from Shanghai Jiao Tong University and the ANP Community provides the first comprehensive analysis of existing agent protocols, offering a systematic two-dimensional classification that distinguishes between context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.</p><p>The paper highlights a critical issue in the rapidly evolving landscape of LLM agents: the lack of standardized protocols for communication with external tools or data sources. This standardization gap makes it difficult for agents to work together effectively or scale across complex tasks, ultimately limiting their potential for tackling real-world problems.</p></div><a href=/posts/survey-ai-agent-protocols/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/ai-agents/ class="link blue hover-black">Tag: Ai-Agents</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/survey-ai-agent-protocols/ class="link black dim">A Survey of AI Agent Protocols: Framework and Future</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>A recent research paper from Shanghai Jiao Tong University and the ANP Community provides the first comprehensive analysis of existing agent protocols, offering a systematic two-dimensional classification that distinguishes between context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.</p><p>The paper highlights a critical issue in the rapidly evolving landscape of LLM agents: the lack of standardized protocols for communication with external tools or data sources. This standardization gap makes it difficult for agents to work together effectively or scale across complex tasks, ultimately limiting their potential for tackling real-world problems.</p></div><a href=/posts/survey-ai-agent-protocols/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/agent-failure/ class="link black dim">Failure Modes of AI Agents: Effects</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Rubén Fernández (@rub) recently shared insights on a Microsoft paper about AI Agent failure modes, concerned it might not get the attention it deserves. You can find his original note here: <a href="https://substack.com/@thelearningrub/note/c-113284290?utm_source=notes-share-action&amp;r=dhjup">https://substack.com/@thelearningrub/note/c-113284290</a></p><p>He mentioned:</p><blockquote><p>I liked Microsoft&rsquo;s paper about Failure Modes of AI Agents, but I think it will go unnoticed by most people, so I&rsquo;ll prepare small infographics to showcase the information it contains.</p><p>The first one, some Effects of AI Agents&rsquo; failure</p></div><a href=/posts/agent-failure/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/ai-ecosystem/ class="link blue hover-black">Tag: Ai-Ecosystem</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/survey-ai-agent-protocols/ class="link black dim">A Survey of AI Agent Protocols: Framework and Future</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>A recent research paper from Shanghai Jiao Tong University and the ANP Community provides the first comprehensive analysis of existing agent protocols, offering a systematic two-dimensional classification that distinguishes between context-oriented versus inter-agent protocols and general-purpose versus domain-specific protocols.</p><p>The paper highlights a critical issue in the rapidly evolving landscape of LLM agents: the lack of standardized protocols for communication with external tools or data sources. This standardization gap makes it difficult for agents to work together effectively or scale across complex tasks, ultimately limiting their potential for tackling real-world problems.</p></div><a href=/posts/survey-ai-agent-protocols/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/deepmind/ class="link blue hover-black">Tag: Deepmind</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/long-context/ class="link blue hover-black">Tag: Long-Context</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/podcast/ class="link blue hover-black">Tag: Podcast</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/rag/ class="link blue hover-black">Tag: Rag</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/release-notes/ class="link blue hover-black">Tag: Release-Notes</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>May 5, 2025</div><h1 class="f3 near-black"><a href=/posts/long-context-rag-release-notes/ class="link black dim">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></div><a href=/posts/long-context-rag-release-notes/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/failure-modes/ class="link blue hover-black">Tag: Failure-Modes</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/agent-failure/ class="link black dim">Failure Modes of AI Agents: Effects</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Rubén Fernández (@rub) recently shared insights on a Microsoft paper about AI Agent failure modes, concerned it might not get the attention it deserves. You can find his original note here: <a href="https://substack.com/@thelearningrub/note/c-113284290?utm_source=notes-share-action&amp;r=dhjup">https://substack.com/@thelearningrub/note/c-113284290</a></p><p>He mentioned:</p><blockquote><p>I liked Microsoft&rsquo;s paper about Failure Modes of AI Agents, but I think it will go unnoticed by most people, so I&rsquo;ll prepare small infographics to showcase the information it contains.</p><p>The first one, some Effects of AI Agents&rsquo; failure</p></div><a href=/posts/agent-failure/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/infographic/ class="link blue hover-black">Tag: Infographic</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/agent-failure/ class="link black dim">Failure Modes of AI Agents: Effects</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Rubén Fernández (@rub) recently shared insights on a Microsoft paper about AI Agent failure modes, concerned it might not get the attention it deserves. You can find his original note here: <a href="https://substack.com/@thelearningrub/note/c-113284290?utm_source=notes-share-action&amp;r=dhjup">https://substack.com/@thelearningrub/note/c-113284290</a></p><p>He mentioned:</p><blockquote><p>I liked Microsoft&rsquo;s paper about Failure Modes of AI Agents, but I think it will go unnoticed by most people, so I&rsquo;ll prepare small infographics to showcase the information it contains.</p><p>The first one, some Effects of AI Agents&rsquo; failure</p></div><a href=/posts/agent-failure/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/microsoft/ class="link blue hover-black">Tag: Microsoft</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/agent-failure/ class="link black dim">Failure Modes of AI Agents: Effects</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Rubén Fernández (@rub) recently shared insights on a Microsoft paper about AI Agent failure modes, concerned it might not get the attention it deserves. You can find his original note here: <a href="https://substack.com/@thelearningrub/note/c-113284290?utm_source=notes-share-action&amp;r=dhjup">https://substack.com/@thelearningrub/note/c-113284290</a></p><p>He mentioned:</p><blockquote><p>I liked Microsoft&rsquo;s paper about Failure Modes of AI Agents, but I think it will go unnoticed by most people, so I&rsquo;ll prepare small infographics to showcase the information it contains.</p><p>The first one, some Effects of AI Agents&rsquo; failure</p></div><a href=/posts/agent-failure/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/context-caching/ class="link blue hover-black">Tag: Context-Caching</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/gemini-caching/ class="link black dim">Gemini Context Caching Explained</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p></div><a href=/posts/gemini-caching/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/cost-optimization/ class="link blue hover-black">Tag: Cost-Optimization</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/gemini-caching/ class="link black dim">Gemini Context Caching Explained</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p></div><a href=/posts/gemini-caching/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/gemini/ class="link blue hover-black">Tag: Gemini</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/gemini-caching/ class="link black dim">Gemini Context Caching Explained</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p></div><a href=/posts/gemini-caching/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/openai/ class="link blue hover-black">Tag: Openai</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/gemini-caching/ class="link black dim">Gemini Context Caching Explained</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p></div><a href=/posts/gemini-caching/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/prompt-engineering/ class="link blue hover-black">Tag: Prompt-Engineering</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/gemini-caching/ class="link black dim">Gemini Context Caching Explained</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p></div><a href=/posts/gemini-caching/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/collaborative-training/ class="link blue hover-black">Tag: Collaborative-Training</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/intellect-2/ class="link black dim">Intellect-2: First Decentralized 32B RL Training Complete</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Prime Intellect (<a href=https://x.com/PrimeIntellect>@PrimeIntellect</a>) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.</p><figure><img src=/images/intellect2.png alt="Intellect-2 Training Progress"></figure><p><strong>Key Points:</strong></p><ul><li><strong>Milestone:</strong> This marks the first successful decentralized RL training of a 32B model.</li><li><strong>Open Collaboration:</strong> The training was open to compute contributions from anyone, making it fully permissionless.</li><li><strong>Goal:</strong> The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.</li><li><strong>Upcoming Release:</strong> A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).</li><li><strong>Community Effort:</strong> The announcement highlighted the significant contributions from various compute providers, including Demeter<em>compute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr</em>, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.</li></ul><p><strong>Links:</strong></p></div><a href=/posts/intellect-2/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/decentralized-ai/ class="link blue hover-black">Tag: Decentralized-Ai</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/intellect-2/ class="link black dim">Intellect-2: First Decentralized 32B RL Training Complete</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Prime Intellect (<a href=https://x.com/PrimeIntellect>@PrimeIntellect</a>) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.</p><figure><img src=/images/intellect2.png alt="Intellect-2 Training Progress"></figure><p><strong>Key Points:</strong></p><ul><li><strong>Milestone:</strong> This marks the first successful decentralized RL training of a 32B model.</li><li><strong>Open Collaboration:</strong> The training was open to compute contributions from anyone, making it fully permissionless.</li><li><strong>Goal:</strong> The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.</li><li><strong>Upcoming Release:</strong> A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).</li><li><strong>Community Effort:</strong> The announcement highlighted the significant contributions from various compute providers, including Demeter<em>compute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr</em>, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.</li></ul><p><strong>Links:</strong></p></div><a href=/posts/intellect-2/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/large-models/ class="link blue hover-black">Tag: Large-Models</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/intellect-2/ class="link black dim">Intellect-2: First Decentralized 32B RL Training Complete</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Prime Intellect (<a href=https://x.com/PrimeIntellect>@PrimeIntellect</a>) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.</p><figure><img src=/images/intellect2.png alt="Intellect-2 Training Progress"></figure><p><strong>Key Points:</strong></p><ul><li><strong>Milestone:</strong> This marks the first successful decentralized RL training of a 32B model.</li><li><strong>Open Collaboration:</strong> The training was open to compute contributions from anyone, making it fully permissionless.</li><li><strong>Goal:</strong> The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.</li><li><strong>Upcoming Release:</strong> A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).</li><li><strong>Community Effort:</strong> The announcement highlighted the significant contributions from various compute providers, including Demeter<em>compute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr</em>, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.</li></ul><p><strong>Links:</strong></p></div><a href=/posts/intellect-2/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div><h2 class=f1><a href=/tags/reinforcement-learning/ class="link blue hover-black">Tag: Reinforcement-Learning</a></h2><div class="mb3 pa4 mid-gray overflow-hidden"><div class=f6>April 27, 2025</div><h1 class="f3 near-black"><a href=/posts/intellect-2/ class="link black dim">Intellect-2: First Decentralized 32B RL Training Complete</a></h1><div class="nested-links f5 lh-copy nested-copy-line-height"><p>Prime Intellect (<a href=https://x.com/PrimeIntellect>@PrimeIntellect</a>) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.</p><figure><img src=/images/intellect2.png alt="Intellect-2 Training Progress"></figure><p><strong>Key Points:</strong></p><ul><li><strong>Milestone:</strong> This marks the first successful decentralized RL training of a 32B model.</li><li><strong>Open Collaboration:</strong> The training was open to compute contributions from anyone, making it fully permissionless.</li><li><strong>Goal:</strong> The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.</li><li><strong>Upcoming Release:</strong> A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).</li><li><strong>Community Effort:</strong> The announcement highlighted the significant contributions from various compute providers, including Demeter<em>compute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr</em>, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.</li></ul><p><strong>Links:</strong></p></div><a href=/posts/intellect-2/ class="ba b--moon-gray bg-light-gray br2 color-inherit dib f7 hover-bg-moon-gray link mt2 ph2 pv1">read more</a></div></section></div></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://juananpe.github.io/>&copy; GenAI Crew 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>