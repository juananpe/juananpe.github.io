<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Intellect-2: First Decentralized 32B RL Training Complete | GenAI Crew</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Prime Intellect (@PrimeIntellect) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.



Key Points:

Milestone: This marks the first successful decentralized RL training of a 32B model.
Open Collaboration: The training was open to compute contributions from anyone, making it fully permissionless.
Goal: The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.
Upcoming Release: A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).
Community Effort: The announcement highlighted the significant contributions from various compute providers, including Demetercompute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.

Links:"><meta name=generator content="Hugo 0.146.0"><meta name=robots content="index, follow"><meta name=author content="GenAI Crew"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://juananpe.github.io/posts/intellect-2/><meta property="og:url" content="https://juananpe.github.io/posts/intellect-2/"><meta property="og:site_name" content="GenAI Crew"><meta property="og:title" content="Intellect-2: First Decentralized 32B RL Training Complete"><meta property="og:description" content="Prime Intellect (@PrimeIntellect) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.
Key Points:
Milestone: This marks the first successful decentralized RL training of a 32B model. Open Collaboration: The training was open to compute contributions from anyone, making it fully permissionless. Goal: The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science. Upcoming Release: A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024). Community Effort: The announcement highlighted the significant contributions from various compute providers, including Demetercompute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234. Links:"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-27T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-27T00:00:00+00:00"><meta property="article:tag" content="Decentralized-Ai"><meta property="article:tag" content="Reinforcement-Learning"><meta property="article:tag" content="Large-Models"><meta property="article:tag" content="Collaborative-Training"><meta itemprop=name content="Intellect-2: First Decentralized 32B RL Training Complete"><meta itemprop=description content="Prime Intellect (@PrimeIntellect) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.
Key Points:
Milestone: This marks the first successful decentralized RL training of a 32B model. Open Collaboration: The training was open to compute contributions from anyone, making it fully permissionless. Goal: The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science. Upcoming Release: A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024). Community Effort: The announcement highlighted the significant contributions from various compute providers, including Demetercompute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234. Links:"><meta itemprop=datePublished content="2025-04-27T00:00:00+00:00"><meta itemprop=dateModified content="2025-04-27T00:00:00+00:00"><meta itemprop=wordCount content="149"><meta itemprop=keywords content="Decentralized-Ai,Reinforcement-Learning,Large-Models,Collaborative-Training"><meta name=twitter:card content="summary"><meta name=twitter:title content="Intellect-2: First Decentralized 32B RL Training Complete"><meta name=twitter:description content="Prime Intellect (@PrimeIntellect) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.
Key Points:
Milestone: This marks the first successful decentralized RL training of a 32B model. Open Collaboration: The training was open to compute contributions from anyone, making it fully permissionless. Goal: The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science. Upcoming Release: A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024). Community Effort: The announcement highlighted the significant contributions from various compute providers, including Demetercompute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234. Links:"></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=/ class="f3 fw2 hover-white white-90 dib no-underline">GenAI Crew</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="Home page">Home</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/archives/ title="Archives page">Archives</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/tags/ title="Tags page">Tags</a></li></ul><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Intellect-2: First Decentralized 32B RL Training Complete</h1><p class=tracked><strong>GenAI Crew</strong></p><time class="f6 mv4 dib tracked" datetime=2025-04-27T00:00:00Z>April 27, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Prime Intellect (<a href=https://x.com/PrimeIntellect>@PrimeIntellect</a>) announced the completion of INTELLECT-2, the first decentralized Reinforcement Learning (RL) training run for a 32-billion-parameter model.</p><figure><img src=/images/intellect2.png alt="Intellect-2 Training Progress"></figure><p><strong>Key Points:</strong></p><ul><li><strong>Milestone:</strong> This marks the first successful decentralized RL training of a 32B model.</li><li><strong>Open Collaboration:</strong> The training was open to compute contributions from anyone, making it fully permissionless.</li><li><strong>Goal:</strong> The project aims to scale towards frontier reasoning capabilities in areas like coding, math, and science.</li><li><strong>Upcoming Release:</strong> A full open-source release, including model checkpoints, training data, and a detailed technical report, is expected approximately one week after the announcement (made around late August 2024).</li><li><strong>Community Effort:</strong> The announcement highlighted the significant contributions from various compute providers, including Demeter<em>compute, string, BioProtocol, mev_pete, plaintext_cap, skre_0, oldmankotaro, plabs, ibuyrugs, 0xfr</em>, marloXBT, herb0x_, mo, toptickcrypto, cannopo, samsja19, jackminong, and primeprimeint1234.</li></ul><p><strong>Links:</strong></p><ul><li>Completion Announcement: <a href=https://x.com/PrimeIntellect/status/1917295731532259628>https://x.com/PrimeIntellect/status/1917295731532259628</a></li><li>Initial Launch Announcement: <a href=https://x.com/PrimeIntellect/status/1912266266137764307>https://x.com/PrimeIntellect/status/1912266266137764307</a></li></ul><p>This achievement showcases the potential of decentralized approaches for large-scale AI model training.</p><ul class=pa0><li class="list di"><a href=/tags/decentralized-ai/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Decentralized-Ai</a></li><li class="list di"><a href=/tags/reinforcement-learning/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Reinforcement-Learning</a></li><li class="list di"><a href=/tags/large-models/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Large-Models</a></li><li class="list di"><a href=/tags/collaborative-training/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Collaborative-Training</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://juananpe.github.io/>&copy; GenAI Crew 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>