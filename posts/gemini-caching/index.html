<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Gemini Context Caching Explained | GenAI Crew</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.
For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet)."><meta name=generator content="Hugo 0.146.0"><meta name=robots content="index, follow"><meta name=author content="GenAI Crew"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://juananpe.github.io/posts/gemini-caching/><meta property="og:url" content="https://juananpe.github.io/posts/gemini-caching/"><meta property="og:site_name" content="GenAI Crew"><meta property="og:title" content="Gemini Context Caching Explained"><meta property="og:description" content="Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.
For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet)."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-27T10:00:00+00:00"><meta property="article:modified_time" content="2025-04-27T10:00:00+00:00"><meta property="article:tag" content="Gemini"><meta property="article:tag" content="Context-Caching"><meta property="article:tag" content="Cost-Optimization"><meta property="article:tag" content="Openai"><meta property="article:tag" content="Prompt-Engineering"><meta itemprop=name content="Gemini Context Caching Explained"><meta itemprop=description content="Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.
For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet)."><meta itemprop=datePublished content="2025-04-27T10:00:00+00:00"><meta itemprop=dateModified content="2025-04-27T10:00:00+00:00"><meta itemprop=wordCount content="153"><meta itemprop=keywords content="Gemini,Context-Caching,Cost-Optimization,Openai,Prompt-Engineering"><meta name=twitter:card content="summary"><meta name=twitter:title content="Gemini Context Caching Explained"><meta name=twitter:description content="Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.
For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet)."></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=/ class="f3 fw2 hover-white white-90 dib no-underline">GenAI Crew</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="Home page">Home</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/archives/ title="Archives page">Archives</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/tags/ title="Tags page">Tags</a></li></ul><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Gemini Context Caching Explained</h1><p class=tracked><strong>GenAI Crew</strong></p><time class="f6 mv4 dib tracked" datetime=2025-04-27T10:00:00Z>April 27, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Context caching in Gemini allows you to store and pre-compute context, such as documents or even entire code repositories. This cached context can then be reused in subsequent requests, leading to significant cost savings – potentially up to 75%.</p><p>For example, using Gemini 1.5 Pro, caching a full GitHub repository and then asking follow-up questions about it demonstrates this capability. Each subsequent request utilizing the same cache could cost substantially less ($0.31 vs. $1.25 per 1 million tokens, according to the tweet).</p><p>You can find a code example demonstrating this in the following notebook:
<a href=https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-context-caching.ipynb>https://github.com/philschmid/gemini-samples/blob/main/examples/gemini-context-caching.ipynb</a></p><p>Here is an image illustrating the concept:<figure><img src=/images/gemini-caching-code.png alt="Gemini Caching Code"></figure></p><p>This concept is similar to OpenAI&rsquo;s prompt caching feature (<a href=https://openai.com/index/api-prompt-caching/>https://openai.com/index/api-prompt-caching/</a>). With OpenAI, caching is reportedly automatic for prompts exceeding a certain token count (e.g., 1024 tokens), provided the context prefix remains unchanged in subsequent requests. If the prompt is inserted <em>before</em> the cached context, the caching mechanism might not be utilized.</p><ul class=pa0><li class="list di"><a href=/tags/gemini/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Gemini</a></li><li class="list di"><a href=/tags/context-caching/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Context-Caching</a></li><li class="list di"><a href=/tags/cost-optimization/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Cost-Optimization</a></li><li class="list di"><a href=/tags/openai/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Openai</a></li><li class="list di"><a href=/tags/prompt-engineering/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Prompt-Engineering</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://juananpe.github.io/>&copy; GenAI Crew 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>