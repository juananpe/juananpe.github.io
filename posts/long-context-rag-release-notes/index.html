<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast) | GenAI Crew</title>
<meta name=viewport content="width=device-width,minimum-scale=1"><meta name=description content="Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.



Watch the episode:
YouTube Video
Listen to the podcast:
Apple Podcasts | Spotify

Episode Summary

Defining tokens and context windows: What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?
Long context vs. RAG: When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?
Scaling context windows: The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.
Quality improvements: How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.
Practical tips: Context caching, combining RAG with long context, and best practices for developers.
The future: Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.


Chapters

0:00 - Intro
0:52 Introduction & defining tokens
5:27 Context window importance
9:53 RAG vs. Long Context
14:19 Scaling beyond 2 million tokens
18:41 Long context improvements since 1.5 Pro release
23:26 Difficulty of attending to the whole context
28:37 Evaluating long context: beyond needle-in-a-haystack
33:41 Integrating long context research
34:57 Reasoning and long outputs
40:54 Tips for using long context
48:51 The future of long context: near-perfect recall and cost reduction
54:42 The role of infrastructure
56:15 Long-context and agents


Notable Quotes

&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;"><meta name=generator content="Hugo 0.146.0"><meta name=robots content="index, follow"><meta name=author content="GenAI Crew"><link rel=stylesheet href=/ananke/css/main.min.d05fb5f317fcf33b3a52936399bdf6f47dc776516e1692e412ec7d76f4a5faa2.css><link rel=canonical href=https://juananpe.github.io/posts/long-context-rag-release-notes/><meta property="og:url" content="https://juananpe.github.io/posts/long-context-rag-release-notes/"><meta property="og:site_name" content="GenAI Crew"><meta property="og:title" content="Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)"><meta property="og:description" content="Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind’s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what’s next in the field.
Watch the episode:
YouTube Video
Listen to the podcast:
Apple Podcasts | Spotify
Episode Summary Defining tokens and context windows: What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations? Long context vs. RAG: When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval? Scaling context windows: The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed. Quality improvements: How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter. Practical tips: Context caching, combining RAG with long context, and best practices for developers. The future: Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure. Chapters 0:00 - Intro 0:52 Introduction & defining tokens 5:27 Context window importance 9:53 RAG vs. Long Context 14:19 Scaling beyond 2 million tokens 18:41 Long context improvements since 1.5 Pro release 23:26 Difficulty of attending to the whole context 28:37 Evaluating long context: beyond needle-in-a-haystack 33:41 Integrating long context research 34:57 Reasoning and long outputs 40:54 Tips for using long context 48:51 The future of long context: near-perfect recall and cost reduction 54:42 The role of infrastructure 56:15 Long-context and agents Notable Quotes “You can rely on context caching to make it both cheaper and faster to answer.”"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-05-05T10:00:00+00:00"><meta property="article:modified_time" content="2025-05-05T10:00:00+00:00"><meta property="article:tag" content="Long-Context"><meta property="article:tag" content="Rag"><meta property="article:tag" content="Llm"><meta property="article:tag" content="Deepmind"><meta property="article:tag" content="Release-Notes"><meta property="article:tag" content="Podcast"><meta itemprop=name content="Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)"><meta itemprop=description content="Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind’s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what’s next in the field.
Watch the episode:
YouTube Video
Listen to the podcast:
Apple Podcasts | Spotify
Episode Summary Defining tokens and context windows: What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations? Long context vs. RAG: When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval? Scaling context windows: The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed. Quality improvements: How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter. Practical tips: Context caching, combining RAG with long context, and best practices for developers. The future: Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure. Chapters 0:00 - Intro 0:52 Introduction & defining tokens 5:27 Context window importance 9:53 RAG vs. Long Context 14:19 Scaling beyond 2 million tokens 18:41 Long context improvements since 1.5 Pro release 23:26 Difficulty of attending to the whole context 28:37 Evaluating long context: beyond needle-in-a-haystack 33:41 Integrating long context research 34:57 Reasoning and long outputs 40:54 Tips for using long context 48:51 The future of long context: near-perfect recall and cost reduction 54:42 The role of infrastructure 56:15 Long-context and agents Notable Quotes “You can rely on context caching to make it both cheaper and faster to answer.”"><meta itemprop=datePublished content="2025-05-05T10:00:00+00:00"><meta itemprop=dateModified content="2025-05-05T10:00:00+00:00"><meta itemprop=wordCount content="333"><meta itemprop=keywords content="Long-Context,Rag,Llm,Deepmind,Release-Notes,Podcast"><meta name=twitter:card content="summary"><meta name=twitter:title content="Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)"><meta name=twitter:description content="Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind’s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what’s next in the field.
Watch the episode:
YouTube Video
Listen to the podcast:
Apple Podcasts | Spotify
Episode Summary Defining tokens and context windows: What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations? Long context vs. RAG: When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval? Scaling context windows: The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed. Quality improvements: How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter. Practical tips: Context caching, combining RAG with long context, and best practices for developers. The future: Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure. Chapters 0:00 - Intro 0:52 Introduction & defining tokens 5:27 Context window importance 9:53 RAG vs. Long Context 14:19 Scaling beyond 2 million tokens 18:41 Long context improvements since 1.5 Pro release 23:26 Difficulty of attending to the whole context 28:37 Evaluating long context: beyond needle-in-a-haystack 33:41 Integrating long context research 34:57 Reasoning and long outputs 40:54 Tips for using long context 48:51 The future of long context: near-perfect recall and cost reduction 54:42 The role of infrastructure 56:15 Long-context and agents Notable Quotes “You can rely on context caching to make it both cheaper and faster to answer.”"></head><body class="ma0 avenir bg-near-white production"><header><div class=bg-black><nav class="pv3 ph3 ph4-ns" role=navigation><div class="flex-l center items-center justify-between"><a href=/ class="f3 fw2 hover-white white-90 dib no-underline">GenAI Crew</a><div class="flex-l items-center"><ul class="pl0 mr3"><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/ title="Home page">Home</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/archives/ title="Archives page">Archives</a></li><li class="list f5 f4-ns fw4 dib pr3"><a class="hover-white white-90 no-underline" href=/tags/ title="Tags page">Tags</a></li></ul><div class=ananke-socials></div></div></div></nav></div></header><main class=pb7 role=main><article class="flex-l mw8 center ph3 flex-wrap justify-between"><header class="mt4 w-100"><aside class="instapaper_ignoref b helvetica tracked ttu">Posts</aside><div id=sharing class="mt3 ananke-socials"></div><h1 class="f1 athelas mt3 mb1">Long Context Models & RAG: Insights from Google DeepMind (Release Notes Podcast)</h1><p class=tracked><strong>GenAI Crew</strong></p><time class="f6 mv4 dib tracked" datetime=2025-05-05T10:00:00Z>May 5, 2025</time></header><div class="nested-copy-line-height lh-copy serif f4 nested-links mid-gray pr4-l w-two-thirds-l"><p>Explore the synergy between long context models and Retrieval Augmented Generation (RAG) in this episode of the Release Notes podcast. Google DeepMind&rsquo;s Nikolay Savinov joins host Logan Kilpatrick to discuss scaling context windows into the millions, recent quality improvements, RAG versus long context, and what&rsquo;s next in the field.</p><figure><img src=/images/long-context-rag-podcast.png alt="Release Notes Podcast: Long Context & RAG"></figure><p><strong>Watch the episode:</strong><br><a href="https://www.youtube.com/watch?v=NHMJ9mqKeMQ&amp;ab_channel=GoogleforDevelopers">YouTube Video</a></p><p><strong>Listen to the podcast:</strong><br><a href=https://goo.gle/3Bm7QzQ>Apple Podcasts</a> | <a href=https://goo.gle/3ZL3ADl>Spotify</a></p><hr><h2 id=episode-summary>Episode Summary</h2><ul><li><strong>Defining tokens and context windows:</strong> What is a token, and why do LLMs use them? How does tokenization affect model behavior and limitations?</li><li><strong>Long context vs. RAG:</strong> When is RAG still necessary, and how do long context models change the landscape for knowledge retrieval?</li><li><strong>Scaling context windows:</strong> The technical and economic challenges of moving from 1M to 10M+ tokens, and what breakthroughs are needed.</li><li><strong>Quality improvements:</strong> How recent models (Gemini 1.5 Pro, 2.5 Pro) have improved long context quality, and what benchmarks matter.</li><li><strong>Practical tips:</strong> Context caching, combining RAG with long context, and best practices for developers.</li><li><strong>The future:</strong> Predictions for superhuman coding assistants, agentic use cases, and the role of infrastructure.</li></ul><hr><h2 id=chapters>Chapters</h2><ul><li>0:00 - Intro</li><li>0:52 Introduction & defining tokens</li><li>5:27 Context window importance</li><li>9:53 RAG vs. Long Context</li><li>14:19 Scaling beyond 2 million tokens</li><li>18:41 Long context improvements since 1.5 Pro release</li><li>23:26 Difficulty of attending to the whole context</li><li>28:37 Evaluating long context: beyond needle-in-a-haystack</li><li>33:41 Integrating long context research</li><li>34:57 Reasoning and long outputs</li><li>40:54 Tips for using long context</li><li>48:51 The future of long context: near-perfect recall and cost reduction</li><li>54:42 The role of infrastructure</li><li>56:15 Long-context and agents</li></ul><hr><h2 id=notable-quotes>Notable Quotes</h2><blockquote><p>&ldquo;You can rely on context caching to make it both cheaper and faster to answer.&rdquo;</p></blockquote><blockquote><p>&ldquo;The benefit of long context for RAG is that you will be able to retrieve more relevant needles from the context by using RAG.&rdquo;</p></blockquote><blockquote><p>&ldquo;This thing is going to be incredible for coding applications.&rdquo;</p></blockquote><blockquote><p>&ldquo;Agents can be considered both consumers and suppliers for long context.&rdquo;</p></blockquote><hr><p>For a deep dive, watch or listen to the full episode linked above!</p><ul class=pa0><li class="list di"><a href=/tags/long-context/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Long-Context</a></li><li class="list di"><a href=/tags/rag/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Rag</a></li><li class="list di"><a href=/tags/llm/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Llm</a></li><li class="list di"><a href=/tags/deepmind/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Deepmind</a></li><li class="list di"><a href=/tags/release-notes/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Release-Notes</a></li><li class="list di"><a href=/tags/podcast/ class="link f5 grow br-pill ba ph3 pv2 mb2 dib black sans-serif no-underline">Podcast</a></li></ul><div class="mt6 instapaper_ignoref"></div></div><aside class="w-30-l mt6-l"><div class="bg-light-gray pa3 nested-list-reset nested-copy-line-height nested-links"><p class="f5 b mb3">Related</p><ul class="pa0 list"><li class=mb2><a href=/posts/survey-ai-agent-protocols/>A Survey of AI Agent Protocols: Framework and Future</a></li></ul></div></aside></article></main><footer class="bg-black bottom-0 w-100 pa3" role=contentinfo><div class="flex justify-between"><a class="f4 fw4 hover-white white-70 dn dib-ns pv2 ph3 no-underline" href=https://juananpe.github.io/>&copy; GenAI Crew 2025</a><div><div class=ananke-socials></div></div></div></footer></body></html>